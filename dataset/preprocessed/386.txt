import os.path
import tensorflow as tf
import helper
import warnings
from distutils.version import LooseVersion
import project_tests as tests
from matplotlib import pyplot as plt
import time
import shutil
assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)
print('TensorFlow Version: {}'.format(tf.__version__))
if not tf.test.gpu_device_name():
    warnings.warn('No GPU found. Please use a GPU to train your neural network.')
else:
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
num_classes = 3
NUMBER_OF_CLASSES = 3
image_shape = 352, 640
IMAGE_SHAPE = 352, 640
EPOCHS = 40
BATCH_SIZE = 4
DROPOUT = 0.75
data_dir = './data'
test_data_dir = '../data/360_raw/test'
runs_dir = '../data/runs'
training_dir = './data/data_road/training'
vgg_path = './data/vgg'
correct_label = tf.placeholder(tf.float32, [None, IMAGE_SHAPE[0], IMAGE_SHAPE[1], NUMBER_OF_CLASSES])
learning_rate = tf.placeholder(tf.float32)
keep_prob = tf.placeholder(tf.float32)


def load_vgg(sess, vgg_path):
    """
    Load Pretrained VGG Model into TensorFlow.
    :param sess: TensorFlow Session
    :param vgg_path: Path to vgg folder, containing "variables/" and "saved_model.pb"
    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)
    """
    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)
    graph = tf.get_default_graph()
    image_input = graph.get_tensor_by_name('image_input:0')
    keep_prob = graph.get_tensor_by_name('keep_prob:0')
    layer3 = graph.get_tensor_by_name('layer3_out:0')
    layer4 = graph.get_tensor_by_name('layer4_out:0')
    parameters = []
    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='conv5_1/weights')
    conv = tf.nn.conv2d(layer4, kernel, [1, 1, 1, 1], padding='SAME')
    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='conv5_1/biases')
    out = tf.nn.bias_add(conv, biases)
    conv5_1 = tf.nn.relu(out, name='conv5_1')
    parameters += [kernel, biases]
    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='conv5_2/weights')
    conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')
    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='conv5_2/biases')
    out = tf.nn.bias_add(conv, biases)
    conv5_2 = tf.nn.relu(out, 'conv5_2')
    parameters += [kernel, biases]
    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32, stddev=0.1), name='conv5_3/weights')
    conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')
    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32), trainable=True, name='conv5_3/biases')
    out = tf.nn.bias_add(conv, biases)
    conv5_3 = tf.nn.relu(out, name='conv5_3')
    parameters += [kernel, biases]
    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool5')
    kernel = tf.Variable(tf.truncated_normal([11, 20, 512, 32], dtype=tf.float32, stddev=0.1), name='conv6/weights')
    conv = tf.nn.conv2d(pool5, kernel, [1, 1, 1, 1], padding='VALID')
    biases = tf.Variable(tf.constant(0.0, shape=[32], dtype=tf.float32), trainable=True, name='conv6/biases')
    out = tf.nn.bias_add(conv, biases)
    conv6 = tf.nn.relu(out, name='conv6')
    parameters += [kernel, biases]
    kernel = tf.Variable(tf.truncated_normal([1, 1, 32, 4096], dtype=tf.float32, stddev=0.1), name='conv6/weights')
    conv = tf.nn.conv2d(conv6, kernel, [1, 1, 1, 1], padding='VALID')
    biases = tf.Variable(tf.constant(0.0, shape=[4096], dtype=tf.float32), trainable=True, name='conv6/biases')
    out = tf.nn.bias_add(conv, biases)
    conv7 = tf.nn.relu(out, name='conv7')
    parameters += [kernel, biases]
    layer7 = conv7
    return image_input, keep_prob, layer3, layer4, layer7


def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):
    """
    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.
    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output
    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output
    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output
    :param num_classes: Number of classes to classify
    :return: The Tensor for the last layer of output
    """
    layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out
    fcn8 = tf.layers.conv2d(layer7, filters=num_classes, kernel_size=1, name='fcn8')
    fcn9 = tf.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1], kernel_size=4, strides=(2, 2), padding='SAME', name='fcn9')
    fcn9_skip_connected = tf.add(fcn9, layer4, name='fcn9_plus_vgg_layer4')
    fcn10 = tf.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1], kernel_size=4, strides=(2, 2), padding='SAME', name='fcn10_conv2d')
    fcn10_skip_connected = tf.add(fcn10, layer3, name='fcn10_plus_vgg_layer3')
    fcn11 = tf.layers.conv2d_transpose(fcn10_skip_connected, filters=num_classes, kernel_size=16, strides=(8, 8), padding='SAME', name='fcn11')
    print(fcn11.shape.dims)
    print(layer7.shape.dims)
    print(fcn8.shape.dims)
    print(fcn9.shape.dims)
    print(fcn9_skip_connected.shape.dims)
    print(fcn10.shape.dims)
    print(fcn11.shape.dims)
    return fcn11


tests.test_layers(layers)


def optimize(nn_last_layer, correct_label, learning_rate, num_classes):
    """
    Build the TensorFLow loss and optimizer operations.
    :param nn_last_layer: TF Tensor of the last layer in the neural network
    :param correct_label: TF Placeholder for the correct label image
    :param learning_rate: TF Placeholder for the learning rate
    :param num_classes: Number of classes to classify
    :return: Tuple of (logits, train_op, cross_entropy_loss)
    """
    logits = tf.reshape(nn_last_layer, (-1, num_classes), name='fcn_logits')
    correct_label_reshaped = tf.reshape(correct_label, (-1, num_classes))
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.stop_gradient(correct_label_reshaped[:]))
    loss_op = tf.reduce_mean(cross_entropy, name='fcn_loss')
    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op, name='fcn_train_op')
    return logits, correct_label_reshaped, train_op, loss_op


tests.test_optimize(optimize)


def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image, correct_label, keep_prob, learning_rate, correct_labels, logits):
    """
    Train neural network and print out the loss during training.
    :param sess: TF Session
    :param epochs: Number of epochs
    :param batch_size: Batch size
    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)
    :param train_op: TF Operation to train the neural network
    :param cross_entropy_loss: TF Tensor for the amount of loss
    :param input_image: TF Placeholder for input images
    :param correct_label: TF Placeholder for label images
    :param keep_prob: TF Placeholder for dropout keep probability
    :param learning_rate: TF Placeholder for learning rate
    """
    keep_prob_value = 0.5
    learning_rate_value = 0.001
    for epoch in range(epochs):
        counter = 0
        total_loss = 0
        for X_batch, gt_batch in get_batches_fn(batch_size):
            loss, _, cl, lo = sess.run([cross_entropy_loss, train_op, correct_labels, logits], feed_dict={input_image: X_batch, correct_label: gt_batch, keep_prob: keep_prob_value, learning_rate: learning_rate_value})
            total_loss += loss
            counter += 1
        print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()))
        print('EPOCH {} ...'.format(epoch + 1))
        print('Loss = {:.3f}'.format(total_loss))
        print()


def run():
    print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()))
    helper.maybe_download_pretrained_vgg(data_dir)
    print('Pretrained model downloaded')
    with tf.Session() as sess:
        vgg_path = os.path.join(data_dir, 'vgg')
        print(vgg_path)
        get_batches_fn = helper.gen_batch_function('../data', image_shape)
        print('Prepared function to get batches')
        with tf.Session() as session:
            image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, vgg_path)
            print('Get layers form vgg - Done')
            model_output = layers(layer3, layer4, layer7, num_classes)
            print('Added Decoder - Full network architecture On-Line')
            logits, correct_labels, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, num_classes)
            print('Model_optimised')
            print('Initialize all variables - this might slow your computer, just do not worry and wait about 10 min')
            print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()))
            session.run(tf.global_variables_initializer())
            session.run(tf.local_variables_initializer())
            print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()))
            print('Model build successful, starting training')
            train_nn(session, EPOCHS, BATCH_SIZE, get_batches_fn, train_op, cross_entropy_loss, image_input, correct_label, keep_prob, learning_rate, correct_labels, logits)
            print(time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()))
            print('Training Done! :)')
            print('Saving model')
            saver = tf.train.Saver()
            output_dir = '../Trained_Model/' + str(time.strftime('%Y_%m_%d-%H_%M_%S', time.gmtime()))
            if os.path.exists(output_dir):
                shutil.rmtree(output_dir)
            os.makedirs(output_dir)
            saver.save(session, output_dir + '/model.ckpt')
            print('All done!')


if __name__ == '__main__':
    run()
