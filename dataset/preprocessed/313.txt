from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
import scipy.io as sio
import sys, os, os.path, inspect
filedir = inspect.getframeinfo(inspect.currentframe())[0]
filedir = os.path.dirname(os.path.abspath(filedir))
mat_contents = sio.loadmat(filedir + '/sampleData_w4RegionLabels.mat')
train_dataset = np.array(mat_contents['train_data']).astype(np.float32)
train_labels = np.array(mat_contents['train_label']).astype(np.float32)
valid_dataset = np.array(mat_contents['test_data']).astype(np.float32)
valid_labels = np.array(mat_contents['test_label']).astype(np.float32)
print('Training set  - ', train_dataset.shape, train_labels.shape)
print('Test set      - ', valid_dataset.shape, ' ', valid_labels.shape)
sample_size = train_dataset.shape[1]
num_labels = train_labels.shape[1]


def accuracy(predictions, labels):
    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]


batch_size = 128
hidden_size = 512
hidden_1_size = hidden_size
hidden_2_size = hidden_size
hidden_3_size = hidden_size
hidden_4_size = hidden_size
beta = 0.01
SEED = None
dropoutPercent = 0.5
graph = tf.Graph()
with graph.as_default():
    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, sample_size))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_predict_label = tf.placeholder(tf.float32, shape=(1, sample_size))
    weights_1 = tf.Variable(tf.truncated_normal([sample_size, hidden_1_size], stddev=0.1), name='weights_1')
    biases_1 = tf.Variable(tf.zeros([hidden_1_size]), name='biases_1')
    weights_2 = tf.Variable(tf.truncated_normal([hidden_1_size, hidden_2_size]), name='weights_2')
    biases_2 = tf.Variable(tf.zeros([hidden_2_size]), name='biases_2')
    weights_3 = tf.Variable(tf.truncated_normal([hidden_2_size, hidden_3_size]), name='weights_3')
    biases_3 = tf.Variable(tf.zeros([hidden_3_size]), name='biases_3')
    weights_4 = tf.Variable(tf.truncated_normal([hidden_3_size, hidden_4_size]), name='weights_4')
    biases_4 = tf.Variable(tf.zeros([hidden_4_size]), name='biases_4')
    weights_o = tf.Variable(tf.truncated_normal([hidden_4_size, num_labels], stddev=0.1), name='weights_o')
    biases_o = tf.Variable(tf.zeros([num_labels]), name='biases_o')

    def model(data, train=False):
        hidden = tf.nn.relu(tf.matmul(data, weights_1) + biases_1)
        if train:
            hidden = tf.nn.dropout(hidden, dropoutPercent, seed=SEED)
        hidden = tf.nn.relu(tf.matmul(hidden, weights_2) + biases_2)
        if train:
            hidden = tf.nn.dropout(hidden, dropoutPercent, seed=SEED)
        hidden = tf.nn.relu(tf.matmul(hidden, weights_3) + biases_3)
        if train:
            hidden = tf.nn.dropout(hidden, dropoutPercent, seed=SEED)
        return tf.matmul(hidden, weights_o) + biases_o
    logits = model(tf_train_dataset, True)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
    regularized_loss = loss + beta * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(weights_o))
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(0.002, global_step, 250, 0.96)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(regularized_loss, global_step=global_step)
    train_prediction = tf.nn.softmax(model(tf_train_dataset))
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    data_prediction = tf.nn.softmax(model(tf_predict_label))
    init_op = tf.initialize_all_variables()
    saver = tf.train.Saver()
session = tf.Session(graph=graph)


def train(num_steps=15001, save=True, filename='model_regionDNN.ckpt'):
    with session.as_default():
        session.run(init_op)
        print('Initialized')
        for step in range(num_steps):
            offset = step * batch_size % (train_labels.shape[0] - batch_size)
            batch_data = train_dataset[offset:offset + batch_size, :]
            batch_labels = train_labels[offset:offset + batch_size, :]
            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}
            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)
            if step % 500 == 0:
                print('Minibatch loss at step %d: %f  Learning_rate: %f' % (step, l, learning_rate.eval()))
                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))
                save_path = saver.save(session, filedir + '/' + filename, global_step=step)
        if save:
            save_path = saver.save(session, filedir + '/' + filename)


def init():
    with session.as_default():
        saver.restore(session, filedir + '/model_regionDNN.ckpt')


def predict(input_data):
    with session.as_default():
        feed_dict = {tf_predict_label: input_data}
        predictionArray = session.run([data_prediction], feed_dict=feed_dict)
        prediction = np.argmax(predictionArray)
        print('Region-Prediction: {}, np.argmax: {}'.format(predictionArray, prediction))
        return prediction


def close():
    session.close()


if __name__ == '__main__':
    train(save=True)
    close()
